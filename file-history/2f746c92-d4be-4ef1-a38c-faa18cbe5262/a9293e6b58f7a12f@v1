"""
Test basic crawling (title, author, URL only) and DB save
"""
import asyncio
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "."))

from playwright.async_api import async_playwright
from app.services.post_service import save_posts, get_recent_posts


async def simple_crawl():
    """Simple crawler that only gets list data (no detail pages)"""
    url = "https://cafe.naver.com/f-e/cafes/28866679/menus/1?viewType=L"

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        page = await context.new_page()

        try:
            print(f"Loading: {url}")
            await page.goto(url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(3)

            # Extract just the list data
            list_items = await page.evaluate("""
                () => {
                    const posts = [];
                    const tableSelectors = [
                        '.ArticleListTable tbody tr',
                        '.article-board table tbody tr',
                        '.board-list tbody tr',
                        '.article-table tbody tr'
                    ];

                    let rows = [];
                    for (const selector of tableSelectors) {
                        rows = Array.from(document.querySelectorAll(selector));
                        if (rows.length > 0) break;
                    }

                    rows.forEach(row => {
                        const titleEl = row.querySelector('a.article') ||
                                      row.querySelector('.article_title a') ||
                                      row.querySelector('td.title a');

                        const authorEl = row.querySelector('.nickname') ||
                                       row.querySelector('.p-nick') ||
                                       row.querySelector('.writer');

                        const dateEl = row.querySelector('.type_date') ||
                                     row.querySelector('.date') ||
                                     row.querySelector('.td_date');

                        if (titleEl && titleEl.href) {
                            let link = titleEl.href;
                            if (link && !link.startsWith('http')) {
                                link = 'https://cafe.naver.com' + link;
                            }

                            const isNotice = row.classList.contains('notice') ||
                                           row.classList.contains('top-notice');

                            if (!isNotice) {
                                posts.push({
                                    title: titleEl.innerText.trim(),
                                    link: link,
                                    author: authorEl ? authorEl.innerText.trim() : 'Unknown',
                                    date_str: dateEl ? dateEl.innerText.trim() : '',
                                });
                            }
                        }
                    });
                    return posts;
                }
            """)

            print(f"Found {len(list_items)} posts")

            # Convert to our format
            results = []
            for item in list_items[:10]:  # Limit to first 10
                # Extract article ID from URL
                import re
                match = re.search(r'/articles/(\d+)', item['link'])
                if match:
                    external_id = f"naver_cafe_28866679_{match.group(1)}"
                    results.append({
                        "source": "naver_cafe",
                        "external_id": external_id,
                        "title": item['title'],
                        "url": item['link'],
                        "author": item['author'],
                        "content": "",  # Empty for now
                        "comments": [],  # Empty for now
                        "created_at": item['date_str'],
                        "collected_at": datetime.utcnow(),
                    })

            return results

        finally:
            await browser.close()


async def main():
    print("=== Testing Basic Crawl and DB Save ===\n")

    # Step 1: Crawl (list only, no detail pages)
    print("Step 1: Crawling post list...")
    posts = await simple_crawl()

    print(f"\nCrawled {len(posts)} posts\n")

    if posts:
        print("First 3 posts:")
        for i, post in enumerate(posts[:3], 1):
            print(f"{i}. {post['title'][:60]}")
            print(f"   Author: {post['author']}")
            print(f"   URL: {post['url']}")
            print(f"   External ID: {post['external_id']}\n")

    # Step 2: Save to DB
    print("\nStep 2: Saving to database...")
    save_result = await save_posts(posts)
    print(f"Save result: {save_result}\n")

    # Step 3: Verify from DB
    print("Step 3: Retrieving from database...")
    db_posts = await get_recent_posts(limit=10)
    print(f"Retrieved {len(db_posts)} posts from DB\n")

    if db_posts:
        for i, post in enumerate(db_posts[:5], 1):
            print(f"{i}. {post.title[:60]}")
            print(f"   Author: {post.author}")
            print(f"   External ID: {post.external_id}")
            print(f"   Collected at: {post.collected_at}\n")

    print("=== Test Complete! ===")
    print(f"\n✅ Successfully crawled {len(posts)} posts and saved to DB!")
    print("✅ Basic data collection (title, author, URL) is working!")
    print("\n⚠️  Note: Content and comments collection needs improvement (timeout issues)")


if __name__ == "__main__":
    asyncio.run(main())
