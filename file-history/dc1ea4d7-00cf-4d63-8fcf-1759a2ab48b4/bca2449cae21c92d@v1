# ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

## 1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 1.1 High-Level Architecture

```mermaid
graph TB
    User[ì‚¬ìš©ì] -->|Slack OAuth| Frontend[Next.js Frontend<br/>Vercel]
    Frontend -->|HTTPS API| Backend[FastAPI Backend<br/>GCP Cloud Run]

    subgraph "GCP Cloud Run"
        Backend -->|CRUD| DB[(Supabase<br/>PostgreSQL)]
        Backend -->|Cache| Redis[(Upstash<br/>Redis)]

        Scheduler[Celery Beat] -->|Trigger| Crawler[Reddit Crawler]
        Crawler -->|PRAW| RedditAPI[Reddit API]
        Crawler -->|Save| DB

        Scheduler -->|Trigger| AIAgent[AI Filter Agent]
        AIAgent -->|Query| DB
        AIAgent -->|Prompt| LLM[OpenAI GPT-4o]
        AIAgent -->|Similarity| Embeddings[OpenAI<br/>Embeddings]

        AIAgent -->|Selected Posts| Notifier[Slack Notifier]
    end

    Notifier -->|Post Message| Slack[Slack Workspace]
    Slack -->|Notification| User

    style Frontend fill:#61dafb
    style Backend fill:#009688
    style DB fill:#3ecf8e
    style Redis fill:#dc382d
    style LLM fill:#10a37f
```

### 1.2 Component Interaction

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as FastAPI
    participant C as Celery
    participant R as Reddit
    participant AI as OpenAI
    participant S as Slack
    participant DB as Supabase

    U->>F: ì„¤ì • ì…ë ¥ (í”„ë¡¬í”„íŠ¸, ì±„ë„)
    F->>A: POST /api/config
    A->>DB: Save user settings

    Note over C: Scheduled Task (Cron)
    C->>A: Trigger crawl job
    A->>R: Fetch posts (PRAW)
    R-->>A: Posts + Comments
    A->>DB: Save raw posts

    A->>AI: Filter posts (prompt)
    AI-->>A: Relevance scores
    A->>AI: Generate embeddings
    AI-->>A: Vectors
    A->>DB: Save filtered posts

    A->>S: Send messages (ê°œë³„)
    S-->>U: Slack notification

    U->>F: View dashboard
    F->>A: GET /api/history
    A->>DB: Query filtered_posts
    DB-->>F: Display results
```

---

## 2. ë””ë ‰í† ë¦¬ êµ¬ì¡°

### 2.1 Monorepo Structure

```
sentiment-curator/
â”œâ”€â”€ frontend/                 # Next.js Application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/              # App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ (auth)/       # Auth routes
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ callback/
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/    # Dashboard page
â”‚   â”‚   â”‚   â”œâ”€â”€ settings/     # Configuration page
â”‚   â”‚   â”‚   â””â”€â”€ api/          # API Route Handlers
â”‚   â”‚   â”‚       â””â”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ components/       # React Components
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/           # Shadcn components
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â””â”€â”€ settings/
â”‚   â”‚   â”œâ”€â”€ lib/              # Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ api-client.ts # Backend API client
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts       # NextAuth config
â”‚   â”‚   â”‚   â””â”€â”€ utils.ts
â”‚   â”‚   â””â”€â”€ types/            # TypeScript types
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ .env.local
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/                  # FastAPI Application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # Entry point
â”‚   â”‚   â”œâ”€â”€ config.py         # Settings (pydantic)
â”‚   â”‚   â”œâ”€â”€ api/              # API Endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py       # OAuth verification
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py     # User settings CRUD
â”‚   â”‚   â”‚   â”œâ”€â”€ crawler.py    # Manual trigger
â”‚   â”‚   â”‚   â””â”€â”€ history.py    # Dashboard data
â”‚   â”‚   â”œâ”€â”€ core/             # Core modules
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py   # SQLAlchemy setup
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py   # Token encryption
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py # Celery config
â”‚   â”‚   â”œâ”€â”€ models/           # SQLAlchemy Models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”‚   â”œâ”€â”€ post.py
â”‚   â”‚   â”‚   â”œâ”€â”€ comment.py
â”‚   â”‚   â”‚   â””â”€â”€ filtered_post.py
â”‚   â”‚   â”œâ”€â”€ schemas/          # Pydantic Schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”‚   â”œâ”€â”€ post.py
â”‚   â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ services/         # Business Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ reddit_crawler.py   # PRAW integration
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_filter.py        # OpenAI integration
â”‚   â”‚   â”‚   â”œâ”€â”€ slack_notifier.py   # Slack SDK
â”‚   â”‚   â”‚   â””â”€â”€ embedding_search.py # Vector similarity
â”‚   â”‚   â””â”€â”€ tasks/            # Celery Tasks
â”‚   â”‚       â”œâ”€â”€ crawl.py      # Scheduled crawl
â”‚   â”‚       â”œâ”€â”€ filter.py     # AI filtering
â”‚   â”‚       â””â”€â”€ cleanup.py    # 48h cleanup
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ alembic/              # DB Migrations
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ REQUIREMENTS.md
â”‚   â”œâ”€â”€ TECH_STACK.md
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # This file
â”‚   â””â”€â”€ API.md                # API Spec
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml        # CI/CD
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml        # Local development
```

---

## 3. ë°ì´í„° í”Œë¡œìš°

### 3.1 í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸

```mermaid
flowchart LR
    A[Celery Beat<br/>Cron Trigger] --> B[Crawl Task]
    B --> C{Get User<br/>Configs}
    C --> D[PRAW Client]
    D --> E[Fetch Posts<br/>r/PUBATTLEGROUNDS]
    E --> F{For each post}
    F --> G[Fetch Comments]
    G --> H[Save to DB]
    H --> I{More posts?}
    I -->|Yes| F
    I -->|No| J[Update Job Status]
    J --> K[Trigger AI Filter]
```

**ì„¸ë¶€ ë‹¨ê³„:**
1. **Cron ìŠ¤ì¼€ì¤„ íŠ¸ë¦¬ê±°** (ì˜ˆ: ë§¤ 6ì‹œê°„)
2. **User Config ì¡°íšŒ** (Supabase)
   - ëŒ€ìƒ ì„œë¸Œë ˆë”§
   - ìˆ˜ì§‘ ê¸°ê°„ (48ì‹œê°„)
3. **Reddit API í˜¸ì¶œ** (PRAW)
   ```python
   subreddit = reddit.subreddit("PUBATTLEGROUNDS")
   for post in subreddit.new(limit=100):
       # 48ì‹œê°„ ì´ë‚´ í•„í„°ë§
       if post.created_utc > cutoff_time:
           save_post(post)
           # ëŒ“ê¸€ë„ ìˆ˜ì§‘
           post.comments.replace_more(limit=0)
           for comment in post.comments.list():
               save_comment(comment)
   ```
4. **DB ì €ì¥** (posts, comments í…Œì´ë¸”)
5. **ì¤‘ë³µ ì²´í¬** (reddit_id UNIQUE)

### 3.2 AI í•„í„°ë§ íŒŒì´í”„ë¼ì¸

```mermaid
flowchart TD
    A[Filter Task] --> B[Get Unfiltered<br/>Posts]
    B --> C[Get User Prompt]
    C --> D{For each post}
    D --> E[Combine Post +<br/>Comments]
    E --> F[Call GPT-4o<br/>Relevance Check]
    F --> G{Relevance > 0.7?}
    G -->|Yes| H[Generate Summary]
    G -->|No| D
    H --> I[Generate Embedding]
    I --> J[Find Similar Posts<br/>Cosine Similarity]
    J --> K[Save Filtered Post]
    K --> D
    D --> L{All processed?}
    L -->|Yes| M[Sort by Score]
    M --> N[Select Top N]
    N --> O[Trigger Notifier]
```

**AI í˜¸ì¶œ ìµœì í™”:**
- **Batch Processing:** ì—¬ëŸ¬ í¬ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì „ì†¡
- **Prompt Caching:** ë™ì¼ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš© ì‹œ ìºì‹±
- **Model Selection:**
  - í•„í„°ë§: `gpt-4o-mini` (ë¹ ë¥´ê³  ì €ë ´)
  - ìš”ì•½: `gpt-4o` (ê³ í’ˆì§ˆ)

**í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ:**
```python
system_prompt = """
You are an AI that filters PUBG community posts based on user interests.

User Interest: {user_prompt}

Given a post (title + content + top comments), determine:
1. Relevance Score (0.0-1.0)
2. Brief summary (2-3 sentences in Korean)

Respond in JSON:
{
  "relevance": 0.85,
  "summary": "ìƒˆë¡œìš´ íŒ¨ì¹˜ë¡œ ì¸í•œ FPS ì €í•˜ ë¬¸ì œê°€ ë³´ê³ ë˜ê³  ìˆìŠµë‹ˆë‹¤..."
}
"""

user_message = f"""
Title: {post.title}
Content: {post.content}
Top Comments: {top_comments}
"""
```

### 3.3 Slack ì•Œë¦¼ í”Œë¡œìš°

```mermaid
flowchart LR
    A[Notifier Task] --> B[Get Top N<br/>Filtered Posts]
    B --> C{For each post}
    C --> D[Translate to Korean<br/>if English]
    D --> E[Build Block Kit<br/>Message]
    E --> F[Post to Slack<br/>Channel]
    F --> G[Save slack_ts]
    G --> H{More posts?}
    H -->|Yes| C
    H -->|No| I[Mark as Sent]
```

**Slack Block Kit ì˜ˆì‹œ:**
```json
{
  "blocks": [
    {
      "type": "header",
      "text": {
        "type": "plain_text",
        "text": "ğŸ® PUBG ì‹ ê·œ í¬ìŠ¤íŠ¸"
      }
    },
    {
      "type": "section",
      "text": {
        "type": "mrkdwn",
        "text": "*ìš”ì•½:*\nìƒˆë¡œìš´ íŒ¨ì¹˜ë¡œ ì¸í•œ FPS ì €í•˜ ë¬¸ì œê°€ ë³´ê³ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ì €ë“¤ì€ íŠ¹íˆ Erangel ë§µì—ì„œ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²½í—˜í•˜ê³  ìˆìœ¼ë©°..."
      }
    },
    {
      "type": "section",
      "fields": [
        {
          "type": "mrkdwn",
          "text": "*ğŸ“ ì›ë³¸:*\n<https://reddit.com/r/PUBATTLEGROUNDS/...|View Post>"
        },
        {
          "type": "mrkdwn",
          "text": "*ğŸ”— ê´€ë ¨ í¬ìŠ¤íŠ¸:*\nâ€¢ <link1|Post 1>\nâ€¢ <link2|Post 2>"
        }
      ]
    },
    {
      "type": "divider"
    }
  ]
}
```

---

## 4. ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ìƒì„¸

### 4.1 Reddit Crawler Service

**ì—­í• :**
- Reddit APIë¥¼ í†µí•œ í¬ìŠ¤íŠ¸/ëŒ“ê¸€ ìˆ˜ì§‘
- Rate Limit ê´€ë¦¬
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„

**í•µì‹¬ ë¡œì§:**
```python
class RedditCrawler:
    def __init__(self):
        self.reddit = praw.Reddit(...)

    async def crawl_subreddit(
        self,
        subreddit_name: str,
        hours_ago: int = 48
    ) -> List[Post]:
        cutoff = datetime.now(timezone.utc) - timedelta(hours=hours_ago)
        subreddit = self.reddit.subreddit(subreddit_name)

        posts = []
        for submission in subreddit.new(limit=200):
            if submission.created_utc < cutoff.timestamp():
                break

            post_data = {
                "reddit_id": submission.id,
                "title": submission.title,
                "content": submission.selftext,
                "url": submission.url,
                "created_at": datetime.fromtimestamp(
                    submission.created_utc,
                    timezone.utc
                ),
                # ...
            }

            # ëŒ“ê¸€ ìˆ˜ì§‘
            submission.comments.replace_more(limit=0)
            comments = [
                {
                    "reddit_id": comment.id,
                    "body": comment.body,
                    "author": comment.author.name if comment.author else "[deleted]",
                    # ...
                }
                for comment in submission.comments.list()
            ]

            posts.append({"post": post_data, "comments": comments})

        return posts
```

### 4.2 AI Filter Service

**ì—­í• :**
- LLMì„ í™œìš©í•œ í¬ìŠ¤íŠ¸ í•„í„°ë§
- ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ í¬ìŠ¤íŠ¸ ê²€ìƒ‰
- ë²ˆì—­ ë° ìš”ì•½ ìƒì„±

**í•µì‹¬ ë¡œì§:**
```python
class AIFilterService:
    def __init__(self):
        self.openai = OpenAI()

    async def filter_post(
        self,
        post: Post,
        comments: List[Comment],
        user_prompt: str
    ) -> FilterResult:
        # 1. Relevance Check
        combined_text = f"{post.title}\n\n{post.content}\n\n"
        combined_text += "\n".join([c.body for c in comments[:10]])

        response = await self.openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": FILTER_SYSTEM_PROMPT},
                {"role": "user", "content": f"User Interest: {user_prompt}\n\nPost:\n{combined_text}"}
            ],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        if result["relevance"] < 0.7:
            return None

        # 2. Generate Summary
        summary = await self._generate_summary(post, comments)

        # 3. Find Related Posts
        embedding = await self._get_embedding(combined_text)
        related = await self._find_similar_posts(embedding, exclude=post.id)

        return FilterResult(
            relevance=result["relevance"],
            summary=summary,
            related_posts=related
        )

    async def _get_embedding(self, text: str) -> List[float]:
        response = await self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    async def _find_similar_posts(
        self,
        query_embedding: List[float],
        exclude: str,
        limit: int = 3
    ) -> List[Post]:
        # PostgreSQL pgvector ì‚¬ìš©
        similar = await db.execute(
            """
            SELECT id, title,
                   1 - (embedding <=> :query) AS similarity
            FROM posts
            WHERE id != :exclude
            ORDER BY embedding <=> :query
            LIMIT :limit
            """,
            {"query": query_embedding, "exclude": exclude, "limit": limit}
        )
        return similar.fetchall()
```

### 4.3 Slack Notifier Service

**ì—­í• :**
- Slack APIë¥¼ í†µí•œ ë©”ì‹œì§€ ì „ì†¡
- Block Kit í¬ë§·íŒ…
- ì—ëŸ¬ í•¸ë“¤ë§

**í•µì‹¬ ë¡œì§:**
```python
class SlackNotifier:
    def __init__(self):
        self.client = WebClient(token=settings.SLACK_BOT_TOKEN)

    async def send_post(
        self,
        channel: str,
        post: FilteredPost
    ) -> str:
        blocks = self._build_blocks(post)

        try:
            response = self.client.chat_postMessage(
                channel=channel,
                text=post.summary[:100],  # Fallback text
                blocks=blocks,
                unfurl_links=False
            )
            return response["ts"]  # Slack timestamp
        except SlackApiError as e:
            logger.error(f"Slack error: {e.response['error']}")
            raise

    def _build_blocks(self, post: FilteredPost) -> List[dict]:
        blocks = [
            {
                "type": "header",
                "text": {"type": "plain_text", "text": "ğŸ® PUBG ì‹ ê·œ í¬ìŠ¤íŠ¸"}
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*ìš”ì•½:*\n{post.summary}"
                }
            },
            {
                "type": "section",
                "fields": [
                    {
                        "type": "mrkdwn",
                        "text": f"*ğŸ“ ì›ë³¸:*\n<{post.url}|View Post>"
                    }
                ]
            }
        ]

        if post.related_posts:
            related_text = "\n".join([
                f"â€¢ <{p.url}|{p.title[:50]}>"
                for p in post.related_posts
            ])
            blocks[-1]["fields"].append({
                "type": "mrkdwn",
                "text": f"*ğŸ”— ê´€ë ¨ í¬ìŠ¤íŠ¸:*\n{related_text}"
            })

        blocks.append({"type": "divider"})
        return blocks
```

### 4.4 Celery Tasks

**ìŠ¤ì¼€ì¤„ ì„¤ì •:**
```python
# app/core/celery_app.py
from celery import Celery
from celery.schedules import crontab

celery = Celery("sentiment_curator")

celery.conf.beat_schedule = {
    "crawl-reddit-every-6h": {
        "task": "app.tasks.crawl.crawl_all_subreddits",
        "schedule": crontab(minute=0, hour="*/6"),  # ë§¤ 6ì‹œê°„
    },
    "cleanup-old-posts-daily": {
        "task": "app.tasks.cleanup.delete_old_posts",
        "schedule": crontab(hour=0, minute=0),  # ë§¤ì¼ ìì •
    },
}
```

**íƒœìŠ¤í¬ ì •ì˜:**
```python
# app/tasks/crawl.py
@celery.task
async def crawl_all_subreddits():
    users = await get_active_users()

    for user in users:
        subreddits = user.settings.get("subreddits", ["PUBATTLEGROUNDS"])

        for subreddit in subreddits:
            await crawl_and_filter.delay(user.id, subreddit)

@celery.task
async def crawl_and_filter(user_id: str, subreddit: str):
    # 1. Crawl
    crawler = RedditCrawler()
    posts = await crawler.crawl_subreddit(subreddit, hours_ago=48)
    await save_posts_to_db(posts)

    # 2. Filter
    ai_filter = AIFilterService()
    user = await get_user(user_id)

    filtered = []
    for post in posts:
        result = await ai_filter.filter_post(
            post,
            post.comments,
            user.settings["prompt"]
        )
        if result:
            filtered.append(result)

    # 3. Select Top N
    top_posts = sorted(filtered, key=lambda x: x.relevance, reverse=True)
    top_posts = top_posts[:user.settings.get("num_posts", 5)]

    # 4. Notify
    if user.settings.get("delivery_mode") == "scheduled":
        notifier = SlackNotifier()
        for post in top_posts:
            await notifier.send_post(user.settings["slack_channel"], post)
```

---

## 5. ì¸ì¦ ë° ë³´ì•ˆ

### 5.1 OAuth Flow (Slack)

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant NA as NextAuth
    participant S as Slack
    participant B as Backend

    U->>F: Click "Login with Slack"
    F->>NA: Initiate OAuth
    NA->>S: Redirect to Slack OAuth
    S->>U: Login & Authorize
    U->>S: Approve
    S->>NA: Callback with code
    NA->>S: Exchange code for token
    S-->>NA: Access Token
    NA->>B: Verify & Save Token
    B->>DB: Store encrypted token
    NA-->>F: Set session cookie
    F->>U: Redirect to Dashboard
```

**NextAuth ì„¤ì •:**
```typescript
// src/lib/auth.ts
export const authOptions: NextAuthOptions = {
  providers: [
    SlackProvider({
      clientId: process.env.SLACK_CLIENT_ID!,
      clientSecret: process.env.SLACK_CLIENT_SECRET!,
    }),
  ],
  callbacks: {
    async jwt({ token, account }) {
      if (account) {
        token.accessToken = account.access_token;
        token.teamId = account.team_id;
      }
      return token;
    },
    async session({ session, token }) {
      session.accessToken = token.accessToken;
      return session;
    },
  },
};
```

### 5.2 API ì¸ì¦

**Frontend â†’ Backend:**
```typescript
// Frontend API Client
const apiClient = {
  async request(endpoint: string, options?: RequestInit) {
    const session = await getServerSession(authOptions);

    return fetch(`${process.env.NEXT_PUBLIC_API_URL}${endpoint}`, {
      ...options,
      headers: {
        "Authorization": `Bearer ${session?.accessToken}`,
        "Content-Type": "application/json",
        ...options?.headers,
      },
    });
  },
};
```

**Backend ê²€ì¦:**
```python
# app/api/dependencies.py
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def get_current_user(
    token: str = Depends(security)
) -> User:
    # Slack token ê²€ì¦
    slack_client = WebClient(token=token.credentials)
    try:
        auth_test = slack_client.auth_test()
        user_id = auth_test["user_id"]
    except SlackApiError:
        raise HTTPException(status_code=401, detail="Invalid token")

    # DBì—ì„œ ì‚¬ìš©ì ì¡°íšŒ
    user = await db.get_user_by_slack_id(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    return user
```

### 5.3 ë¯¼ê° ë°ì´í„° ì•”í˜¸í™”

```python
# app/core/security.py
from cryptography.fernet import Fernet

class TokenEncryption:
    def __init__(self):
        self.cipher = Fernet(settings.ENCRYPTION_KEY.encode())

    def encrypt(self, token: str) -> str:
        return self.cipher.encrypt(token.encode()).decode()

    def decrypt(self, encrypted: str) -> str:
        return self.cipher.decrypt(encrypted.encode()).decode()

# ì‚¬ìš© ì˜ˆì‹œ
encryption = TokenEncryption()
user.access_token = encryption.encrypt(oauth_token)
```

---

## 6. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 6.1 Logging Strategy

**êµ¬ì¡°í™”ëœ ë¡œê¹…:**
```python
import structlog

logger = structlog.get_logger()

logger.info(
    "crawl_completed",
    subreddit="PUBATTLEGROUNDS",
    posts_collected=47,
    duration_seconds=23.5,
    user_id="user-123"
)
```

**ë¡œê·¸ ë ˆë²¨:**
- `DEBUG`: ê°œë°œ í™˜ê²½ ìƒì„¸ ë¡œê·¸
- `INFO`: ì •ìƒ ë™ì‘ (í¬ë¡¤ë§ ì™„ë£Œ, í•„í„°ë§ ì™„ë£Œ ë“±)
- `WARNING`: ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ (Rate Limit ì´ˆê³¼)
- `ERROR`: ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”í•œ ì—ëŸ¬ (API ì‹¤íŒ¨)

### 6.2 Metrics

**ì¶”ì  í•­ëª©:**
- í¬ë¡¤ë§: ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ ìˆ˜, ì†Œìš” ì‹œê°„
- AI í•„í„°ë§: í•„í„°ë§ìœ¨, API ë¹„ìš©
- Slack ì „ì†¡: ì„±ê³µ/ì‹¤íŒ¨ìœ¨
- DB: ì¿¼ë¦¬ ì„±ëŠ¥, ì—°ê²° ìˆ˜

**êµ¬í˜„ (Prometheus):**
```python
from prometheus_client import Counter, Histogram

crawl_posts_total = Counter(
    "crawl_posts_total",
    "Total posts collected",
    ["subreddit"]
)

ai_filter_duration = Histogram(
    "ai_filter_duration_seconds",
    "AI filtering duration"
)

# ì‚¬ìš©
crawl_posts_total.labels(subreddit="PUBATTLEGROUNDS").inc(47)
```

### 6.3 Error Tracking (Sentry)

```python
import sentry_sdk

sentry_sdk.init(
    dsn=settings.SENTRY_DSN,
    environment="production",
    traces_sample_rate=0.1,
)

# ìë™ìœ¼ë¡œ ì—ëŸ¬ ìº¡ì²˜
# ìˆ˜ë™ ìº¡ì²˜
sentry_sdk.capture_exception(exception)
```

---

## 7. ë°°í¬ ì•„í‚¤í…ì²˜

### 7.1 Production Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cloudflare CDN                    â”‚
â”‚                 (DNS + DDoS Protection)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚
        â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vercel     â”‚    â”‚  GCP Cloud Run  â”‚
â”‚  (Frontend)  â”‚â—„â”€â”€â”€â”¤   (Backend)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        â”‚        â”‚
                    â–¼        â–¼        â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚Supabaseâ”‚  â”‚Upstashâ”‚  â”‚OpenAI â”‚
            â”‚  (DB)  â”‚  â”‚Redis â”‚  â”‚ API   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]

jobs:
  deploy-frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: vercel/action@v2
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}

  deploy-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          service_account_key: ${{ secrets.GCP_SA_KEY }}

      - name: Build and Push
        run: |
          gcloud builds submit --tag gcr.io/$PROJECT_ID/api

      - name: Deploy to Cloud Run
        run: |
          gcloud run deploy sentiment-curator-api \
            --image gcr.io/$PROJECT_ID/api \
            --platform managed \
            --region us-central1 \
            --allow-unauthenticated
```

### 7.3 Environment Variables

**Frontend (.env.local):**
```bash
NEXT_PUBLIC_API_URL=https://api.sentiment-curator.com
NEXTAUTH_URL=https://sentiment-curator.com
NEXTAUTH_SECRET=<random-secret>
SLACK_CLIENT_ID=<slack-app-id>
SLACK_CLIENT_SECRET=<slack-secret>
```

**Backend (.env):**
```bash
DATABASE_URL=postgresql+asyncpg://user:pass@db.supabase.co/postgres
REDIS_URL=redis://default:pass@upstash-redis.com:6379
OPENAI_API_KEY=sk-...
SLACK_BOT_TOKEN=xoxb-...
REDDIT_CLIENT_ID=<reddit-id>
REDDIT_CLIENT_SECRET=<reddit-secret>
ENCRYPTION_KEY=<fernet-key>
SENTRY_DSN=https://...
```

---

## 8. í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### 8.1 ìˆ˜í‰ í™•ì¥

**Cloud Run Auto-Scaling:**
```yaml
# cloudrun.yaml
spec:
  template:
    metadata:
      annotations:
        autoscaling.knative.dev/minScale: "1"
        autoscaling.knative.dev/maxScale: "10"
    spec:
      containerConcurrency: 80
```

### 8.2 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

**ì¸ë±ìŠ¤:**
```sql
CREATE INDEX idx_posts_created_at ON posts(created_at DESC);
CREATE INDEX idx_posts_subreddit ON posts(subreddit);
CREATE INDEX idx_filtered_posts_user ON filtered_posts(user_id);

-- Vector ê²€ìƒ‰ (pgvector)
CREATE INDEX ON posts USING ivfflat (embedding vector_cosine_ops);
```

### 8.3 ìºì‹± ì „ëµ

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_user_settings(user_id: str):
    return db.query(User).filter(User.id == user_id).first().settings
```

---

## 9. í–¥í›„ ê°œì„  ë°©í–¥

1. **ì‹¤ì‹œê°„ í¬ë¡¤ë§:** Reddit Stream API í™œìš©
2. **ë©€í‹° í…Œë„ŒíŠ¸:** ì—¬ëŸ¬ íŒ€ì´ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
3. **ì›¹í›…:** Slack ì´ë²¤íŠ¸ êµ¬ë…ìœ¼ë¡œ í”¼ë“œë°± ìˆ˜ì§‘
4. **ê³ ê¸‰ ë¶„ì„:** íŠ¸ë Œë“œ ë¶„ì„, ê°ì„± ë¶„ì„
5. **ëª¨ë°”ì¼ ì•±:** React Nativeë¡œ ëª¨ë°”ì¼ ì§€ì›
